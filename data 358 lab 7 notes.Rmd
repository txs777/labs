---
title: "DATA 358 lab7 notes"
subtitle: "Classification and Regression Tree"
author: "Taylor Stacy"
date: "`r Sys.Date()`"
output:
  html_document:
    number_section: no
    toc: no
    toc_depth: 3
    toc_float: yes
    code_folding: show
    css: lab_templet.css
---

```{r setup, include=FALSE}
rm(list=ls())
library(AmesHousing)
library(tidyverse)
library(caret)
# new packages
library(rpart)
library(rpart.plot)
library(ISLR)
library(rattle)
library(RColorBrewer)
knitr::opts_chunk$set(echo = TRUE)
```
# Classification Tree: Categorical Data
## Data
```{r}
data("Carseats")
glimpse(Carseats)
Carseats <- Carseats %>% 
  mutate(High= as.factor(ifelse(Sales <= 8,
                                "No",
                                "Yes")))
```
## Data Splitting
```{r}
set.seed(123)
inTrain <- createDataPartition(Carseats$High,
                               p=0.8,
                               list = FALSE)
train <- Carseats[inTrain,]
test <- Carseats[-inTrain,]
```
## Train a Classification Tree
```{r}
tree_carseats <- rpart(High ~.-Sales,
                       data = train,
                       method = 'class')
names(tree_carseats)
print(tree_carseats)
```
## Visualizing the Tree
```{r}
fancyRpartPlot(tree_carseats, 
               palette="RdYlGn", 
               sub=" ")
```

This tree has 9 nodes

Is this the optimal tree?

  - Does this tree need pruning?
  - There is a node in tree that will optimize cross validation error 

## Pruning
```{r}
round(tree_carseats$cptable, 4)
# which value has the minimum error
```
The optimal tree has 8 nodes (7 split) with Î± = 0.0227273

## Pruned Tree
```{r}
cp_opt <- tree_carseats$cptable[which.min(tree_carseats$cptable[,"xerror"]),"CP"]
tree_opt <- prune(tree_carseats,
                  cp =cp_opt)
fancyRpartPlot(tree_opt, 
               palette="RdYlGn", 
               sub=" ")
```


## Model Evaluation: Confusion Matrix
```{r}
# Get column index of target variable
ColNum <- grep("High",names(test))

# Make prediction using the fitted model on test data.
# Full tree
full_predict <- predict(tree_carseats,test,type="class")
#Pruned tree
opt_predict <- predict(tree_opt,test,type="class")
```
### Confusion Matrix: Full
```{r}
confusionMatrix(full_predict, test$High)
```
### Confusion Matrix: Pruned
```{r}
confusionMatrix(opt_predict, test$High)
```
# Using the `caret` package

```{r}
train.control <- trainControl(
  method = "repeatedcv",
  number = 10, repeats = 5,             # Needed for AUC
  summaryFunction = twoClassSummary, 
  classProbs = TRUE)
# train model
rpart_caret <- train(High~., data=train, method = "rpart",
                   #tuneLength = 10,
                   trControl = train.control)
# create tree
fancyRpartPlot(rpart_caret$finalModel, 
               palette="RdYlGn", 
               sub=" ")
```

`caret` automatically prunes tree

# Regression Tree: Quantative Data
## Data
```{r}
ames <- make_ames()
```
## Missing Values
```{r}
anyNA(ames)
```
## Data Splitting
```{r}
set.seed(1234)
indxTrain <- ames$Sale_Price %>%
  createDataPartition(p = 0.8, list = FALSE)
train <- ames[indxTrain, ]
test <- ames[-indxTrain, ]
```
## Model Training
```{r}
RegTree_ames <- rpart(Sale_Price ~ ., 
                      data = train, 
                      method="anova",
                      cp=.01)
```
Note: when fitting a regression tree, we need to set **method = "anova"**

## Model Summary and Interpretation
```{r}
print(RegTree_ames)
```
- We start with 2346 observations at the root node. 
- First split was made on a Overall_Qual. 
- 1958 observations with Overall_Qual=Very_Poor, Poor, Fair, Below_Average, Average, Above_Average, Good go to the 2nd (2)) branch. Important statistics are also listed.
- Remaining 388 observations with Overall_Qual = Very_Good, Excellent, Very_Excellent went to the 3rd branch (3))
- This tells us that Overall_Qual is an important predictor of sales price with those homes on the upper end of the quality spectrum having almost double the average sales price.

## Visualizing the Tree
```{r}
fancyRpartPlot(RegTree_ames, palette="RdYlGn", 
               sub=" ")
```

- Default statistics produced are: number and percentage of data that fall in each node, SSE and the predicted outcome for that node.
- The tree has a total of 11 splits resulting in 12 terminal nodes.

## Tree Pruning
```{r}
# plots the Cross validation error for each value of cp
# labels on top x axis are the nodes
plotcp(RegTree_ames)
```

- 1-SE rule: Use the smallest tree within 1 standard error (SE) of the minimum CV error (Breiman, 1984).
- Where the dashed line passes though

## Cross Validation Results
```{r}
RegTree_ames$cptable
# looking for value of cp at minimum x- error that isnt the last node
```

## Parameter Tuning
```{r}
RegTree_ames2 <- rpart(
  Sale_Price ~ .,
  data = train, 
  cp=.01245)
rpart.plot(RegTree_ames2, 
           box.palette="RdYlGn")
```

## Root Mean Squared Error (RMSE)
```{r}
# caret cross validation results
RegTree_ames_c <- train(
  Sale_Price ~ .,
  data = train,
  method = "rpart",
  trControl = trainControl(
    method = "cv", 
    number = 10),
  tuneLength = 20)
ggplot(RegTree_ames_c)
```

## Evaluate Model Performance
```{r}
options(scipen=999)  # turn off scientific notation like 1e+06
#Predict using the fitted tree
predicted<-predict(RegTree_ames, newdata=test)
#conbine fitted and actual values
actual<-test$Sale_Price
actual_pred<-as.data.frame(cbind(predicted,actual))

```
We evaluate the performance of our unpruned tree

## Compute MSE
```{r}
sqrt(mean((predicted-actual)^2))
``` 
This means that the model leads to test predictions that are within around 43000.87 of the true sale price of residential properties in `Ames`

## Visualizing Actual vs. Predicted
```{r}
ggplot(data=actual_pred, 
       aes(y=actual, 
           x=predicted))+
  geom_point()+
  geom_abline()
```






